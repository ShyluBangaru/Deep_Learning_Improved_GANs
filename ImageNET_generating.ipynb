{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_fid\n",
      "  Using cached pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_fid) (1.26.4)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_fid) (10.3.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_fid) (1.13.0)\n",
      "Requirement already satisfied: torch>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_fid) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_fid) (0.21.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.0.1->pytorch_fid) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.0.1->pytorch_fid) (4.11.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.0.1->pytorch_fid) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.0.1->pytorch_fid) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.0.1->pytorch_fid) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.0.1->pytorch_fid) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.0.1->pytorch_fid) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.0.1->pytorch_fid) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.0.1->pytorch_fid) (2.1.5)\n",
      "Using cached pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pytorch_fid\n",
      "Successfully installed pytorch_fid-0.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install pytorch_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch 0/137 D Loss: 1.5993, G Loss: 3.0374\n",
      "Epoch [1/100], Batch 100/137 D Loss: 0.2536, G Loss: 5.1556\n",
      "Epoch [2/100], Batch 0/137 D Loss: 0.8376, G Loss: 2.6610\n",
      "Epoch [2/100], Batch 100/137 D Loss: 0.2998, G Loss: 2.4278\n",
      "Epoch [3/100], Batch 0/137 D Loss: 2.2511, G Loss: 2.3604\n",
      "Epoch [3/100], Batch 100/137 D Loss: 0.4559, G Loss: 2.3719\n",
      "Epoch [4/100], Batch 0/137 D Loss: 0.4341, G Loss: 4.9806\n",
      "Epoch [4/100], Batch 100/137 D Loss: 1.2069, G Loss: 5.3924\n",
      "Epoch [5/100], Batch 0/137 D Loss: 1.1710, G Loss: 4.3157\n",
      "Epoch [5/100], Batch 100/137 D Loss: 0.7046, G Loss: 3.5932\n",
      "Epoch [6/100], Batch 0/137 D Loss: 3.2423, G Loss: 7.1496\n",
      "Epoch [6/100], Batch 100/137 D Loss: 0.3093, G Loss: 4.1285\n",
      "Epoch [7/100], Batch 0/137 D Loss: 0.9963, G Loss: 7.6055\n",
      "Epoch [7/100], Batch 100/137 D Loss: 0.7394, G Loss: 6.3426\n",
      "Epoch [8/100], Batch 0/137 D Loss: 4.0496, G Loss: 5.9850\n",
      "Epoch [8/100], Batch 100/137 D Loss: 0.5409, G Loss: 2.3289\n",
      "Epoch [9/100], Batch 0/137 D Loss: 0.9548, G Loss: 1.7706\n",
      "Epoch [9/100], Batch 100/137 D Loss: 0.7379, G Loss: 2.3024\n",
      "Epoch [10/100], Batch 0/137 D Loss: 0.8996, G Loss: 5.6358\n",
      "Epoch [10/100], Batch 100/137 D Loss: 0.7816, G Loss: 2.0969\n",
      "Epoch [11/100], Batch 0/137 D Loss: 0.9533, G Loss: 4.9365\n",
      "Epoch [11/100], Batch 100/137 D Loss: 0.6567, G Loss: 4.4333\n",
      "Epoch [12/100], Batch 0/137 D Loss: 2.4826, G Loss: 6.3648\n",
      "Epoch [12/100], Batch 100/137 D Loss: 0.2084, G Loss: 4.6496\n",
      "Epoch [13/100], Batch 0/137 D Loss: 1.0952, G Loss: 6.3894\n",
      "Epoch [13/100], Batch 100/137 D Loss: 0.8557, G Loss: 2.2011\n",
      "Epoch [14/100], Batch 0/137 D Loss: 1.8240, G Loss: 7.5059\n",
      "Epoch [14/100], Batch 100/137 D Loss: 0.2927, G Loss: 3.9714\n",
      "Epoch [15/100], Batch 0/137 D Loss: 0.6054, G Loss: 2.3760\n",
      "Epoch [15/100], Batch 100/137 D Loss: 0.6265, G Loss: 2.6582\n",
      "Epoch [16/100], Batch 0/137 D Loss: 0.4790, G Loss: 3.7628\n",
      "Epoch [16/100], Batch 100/137 D Loss: 0.8194, G Loss: 2.6613\n",
      "Epoch [17/100], Batch 0/137 D Loss: 0.7074, G Loss: 1.2402\n",
      "Epoch [17/100], Batch 100/137 D Loss: 0.2885, G Loss: 4.6165\n",
      "Epoch [18/100], Batch 0/137 D Loss: 6.5265, G Loss: 6.1441\n",
      "Epoch [18/100], Batch 100/137 D Loss: 0.3814, G Loss: 3.2753\n",
      "Epoch [19/100], Batch 0/137 D Loss: 3.1149, G Loss: 9.7556\n",
      "Epoch [19/100], Batch 100/137 D Loss: 0.2971, G Loss: 4.1179\n",
      "Epoch [20/100], Batch 0/137 D Loss: 5.1233, G Loss: 8.2625\n",
      "Epoch [20/100], Batch 100/137 D Loss: 0.6382, G Loss: 4.9475\n",
      "Epoch [21/100], Batch 0/137 D Loss: 0.4630, G Loss: 5.1392\n",
      "Epoch [21/100], Batch 100/137 D Loss: 0.2457, G Loss: 4.3561\n",
      "Epoch [22/100], Batch 0/137 D Loss: 0.3907, G Loss: 2.8462\n",
      "Epoch [22/100], Batch 100/137 D Loss: 1.0081, G Loss: 1.3592\n",
      "Epoch [23/100], Batch 0/137 D Loss: 0.3115, G Loss: 3.1260\n",
      "Epoch [23/100], Batch 100/137 D Loss: 0.9805, G Loss: 6.0036\n",
      "Epoch [24/100], Batch 0/137 D Loss: 5.6838, G Loss: 7.9864\n",
      "Epoch [24/100], Batch 100/137 D Loss: 0.4811, G Loss: 1.8649\n",
      "Epoch [25/100], Batch 0/137 D Loss: 0.4496, G Loss: 5.2183\n",
      "Epoch [25/100], Batch 100/137 D Loss: 0.2203, G Loss: 3.8097\n",
      "Epoch [26/100], Batch 0/137 D Loss: 1.1019, G Loss: 7.5059\n",
      "Epoch [26/100], Batch 100/137 D Loss: 0.3701, G Loss: 2.1115\n",
      "Epoch [27/100], Batch 0/137 D Loss: 1.0052, G Loss: 5.2760\n",
      "Epoch [27/100], Batch 100/137 D Loss: 0.6246, G Loss: 5.0614\n",
      "Epoch [28/100], Batch 0/137 D Loss: 7.2314, G Loss: 5.3555\n",
      "Epoch [28/100], Batch 100/137 D Loss: 0.3321, G Loss: 3.7342\n",
      "Epoch [29/100], Batch 0/137 D Loss: 0.1946, G Loss: 4.6786\n",
      "Epoch [29/100], Batch 100/137 D Loss: 0.7283, G Loss: 2.7595\n",
      "Epoch [30/100], Batch 0/137 D Loss: 1.8668, G Loss: 7.8029\n",
      "Epoch [30/100], Batch 100/137 D Loss: 1.0137, G Loss: 6.0512\n",
      "Epoch [31/100], Batch 0/137 D Loss: 4.0823, G Loss: 6.7797\n",
      "Epoch [31/100], Batch 100/137 D Loss: 0.6899, G Loss: 1.6045\n",
      "Epoch [32/100], Batch 0/137 D Loss: 3.6403, G Loss: 8.0603\n",
      "Epoch [32/100], Batch 100/137 D Loss: 0.2622, G Loss: 4.5518\n",
      "Epoch [33/100], Batch 0/137 D Loss: 6.2470, G Loss: 3.5912\n",
      "Epoch [33/100], Batch 100/137 D Loss: 0.2059, G Loss: 3.5200\n",
      "Epoch [34/100], Batch 0/137 D Loss: 0.8047, G Loss: 7.7297\n",
      "Epoch [34/100], Batch 100/137 D Loss: 0.5686, G Loss: 1.5920\n",
      "Epoch [35/100], Batch 0/137 D Loss: 0.6994, G Loss: 7.1829\n",
      "Epoch [35/100], Batch 100/137 D Loss: 0.8521, G Loss: 6.4367\n",
      "Epoch [36/100], Batch 0/137 D Loss: 2.6255, G Loss: 6.5439\n",
      "Epoch [36/100], Batch 100/137 D Loss: 0.1855, G Loss: 3.8136\n",
      "Epoch [37/100], Batch 0/137 D Loss: 1.9805, G Loss: 8.2972\n",
      "Epoch [37/100], Batch 100/137 D Loss: 0.2752, G Loss: 4.2252\n",
      "Epoch [38/100], Batch 0/137 D Loss: 5.8706, G Loss: 5.4783\n",
      "Epoch [38/100], Batch 100/137 D Loss: 0.4522, G Loss: 3.6642\n",
      "Epoch [39/100], Batch 0/137 D Loss: 0.2364, G Loss: 2.9727\n",
      "Epoch [39/100], Batch 100/137 D Loss: 0.2582, G Loss: 3.8077\n",
      "Epoch [40/100], Batch 0/137 D Loss: 5.7834, G Loss: 3.4099\n",
      "Epoch [40/100], Batch 100/137 D Loss: 0.2500, G Loss: 2.5497\n",
      "Epoch [41/100], Batch 0/137 D Loss: 0.3752, G Loss: 4.7008\n",
      "Epoch [41/100], Batch 100/137 D Loss: 0.3411, G Loss: 2.1973\n",
      "Epoch [42/100], Batch 0/137 D Loss: 1.0491, G Loss: 5.9196\n",
      "Epoch [42/100], Batch 100/137 D Loss: 0.0963, G Loss: 3.7786\n",
      "Epoch [43/100], Batch 0/137 D Loss: 7.9157, G Loss: 6.7440\n",
      "Epoch [43/100], Batch 100/137 D Loss: 1.0301, G Loss: 4.7804\n",
      "Epoch [44/100], Batch 0/137 D Loss: 2.9867, G Loss: 7.4420\n",
      "Epoch [44/100], Batch 100/137 D Loss: 0.4188, G Loss: 2.5208\n",
      "Epoch [45/100], Batch 0/137 D Loss: 0.3114, G Loss: 2.7701\n",
      "Epoch [45/100], Batch 100/137 D Loss: 0.4096, G Loss: 4.5454\n",
      "Epoch [46/100], Batch 0/137 D Loss: 0.2915, G Loss: 2.4546\n",
      "Epoch [46/100], Batch 100/137 D Loss: 0.4823, G Loss: 2.8402\n",
      "Epoch [47/100], Batch 0/137 D Loss: 2.3376, G Loss: 9.9002\n",
      "Epoch [47/100], Batch 100/137 D Loss: 0.3258, G Loss: 2.6648\n",
      "Epoch [48/100], Batch 0/137 D Loss: 4.8960, G Loss: 8.8046\n",
      "Epoch [48/100], Batch 100/137 D Loss: 0.4918, G Loss: 4.0322\n",
      "Epoch [49/100], Batch 0/137 D Loss: 3.0408, G Loss: 8.1755\n",
      "Epoch [49/100], Batch 100/137 D Loss: 0.5884, G Loss: 1.4031\n",
      "Epoch [50/100], Batch 0/137 D Loss: 1.4559, G Loss: 9.1671\n",
      "Epoch [50/100], Batch 100/137 D Loss: 0.3121, G Loss: 3.9975\n",
      "Epoch [51/100], Batch 0/137 D Loss: 0.1855, G Loss: 4.7449\n",
      "Epoch [51/100], Batch 100/137 D Loss: 0.1327, G Loss: 4.6261\n",
      "Epoch [52/100], Batch 0/137 D Loss: 0.4076, G Loss: 5.7928\n",
      "Epoch [52/100], Batch 100/137 D Loss: 0.3884, G Loss: 3.5545\n",
      "Epoch [53/100], Batch 0/137 D Loss: 1.0233, G Loss: 7.5035\n",
      "Epoch [53/100], Batch 100/137 D Loss: 0.4871, G Loss: 5.4478\n",
      "Epoch [54/100], Batch 0/137 D Loss: 6.7203, G Loss: 10.6543\n",
      "Epoch [54/100], Batch 100/137 D Loss: 0.5503, G Loss: 3.0665\n",
      "Epoch [55/100], Batch 0/137 D Loss: 0.4192, G Loss: 3.6026\n",
      "Epoch [55/100], Batch 100/137 D Loss: 0.3372, G Loss: 2.6669\n",
      "Epoch [56/100], Batch 0/137 D Loss: 0.5159, G Loss: 4.8276\n",
      "Epoch [56/100], Batch 100/137 D Loss: 0.1588, G Loss: 4.0321\n",
      "Epoch [57/100], Batch 0/137 D Loss: 0.5539, G Loss: 5.1818\n",
      "Epoch [57/100], Batch 100/137 D Loss: 0.3230, G Loss: 4.4432\n",
      "Epoch [58/100], Batch 0/137 D Loss: 5.9752, G Loss: 8.9539\n",
      "Epoch [58/100], Batch 100/137 D Loss: 2.1798, G Loss: 2.5206\n",
      "Epoch [59/100], Batch 0/137 D Loss: 0.0603, G Loss: 4.5416\n",
      "Epoch [59/100], Batch 100/137 D Loss: 0.1771, G Loss: 4.5781\n",
      "Epoch [60/100], Batch 0/137 D Loss: 2.3963, G Loss: 10.1273\n",
      "Epoch [60/100], Batch 100/137 D Loss: 0.3988, G Loss: 5.0320\n",
      "Epoch [61/100], Batch 0/137 D Loss: 1.7579, G Loss: 8.8474\n",
      "Epoch [61/100], Batch 100/137 D Loss: 0.2514, G Loss: 2.6667\n",
      "Epoch [62/100], Batch 0/137 D Loss: 3.9348, G Loss: 13.1061\n",
      "Epoch [62/100], Batch 100/137 D Loss: 0.0548, G Loss: 5.3071\n",
      "Epoch [63/100], Batch 0/137 D Loss: 5.9042, G Loss: 5.1060\n",
      "Epoch [63/100], Batch 100/137 D Loss: 0.3882, G Loss: 1.6840\n",
      "Epoch [64/100], Batch 0/137 D Loss: 1.4027, G Loss: 6.5145\n",
      "Epoch [64/100], Batch 100/137 D Loss: 3.4300, G Loss: 7.9161\n",
      "Epoch [65/100], Batch 0/137 D Loss: 0.1818, G Loss: 3.4080\n",
      "Epoch [65/100], Batch 100/137 D Loss: 0.5017, G Loss: 3.4555\n",
      "Epoch [66/100], Batch 0/137 D Loss: 0.1237, G Loss: 4.1569\n",
      "Epoch [66/100], Batch 100/137 D Loss: 0.1337, G Loss: 6.6197\n",
      "Epoch [67/100], Batch 0/137 D Loss: 1.0627, G Loss: 5.9751\n",
      "Epoch [67/100], Batch 100/137 D Loss: 0.1889, G Loss: 3.9339\n",
      "Epoch [68/100], Batch 0/137 D Loss: 0.2480, G Loss: 2.4541\n",
      "Epoch [68/100], Batch 100/137 D Loss: 0.6769, G Loss: 1.9088\n",
      "Epoch [69/100], Batch 0/137 D Loss: 3.8693, G Loss: 8.2618\n",
      "Epoch [69/100], Batch 100/137 D Loss: 0.0456, G Loss: 4.7953\n",
      "Epoch [70/100], Batch 0/137 D Loss: 4.2378, G Loss: 10.5750\n",
      "Epoch [70/100], Batch 100/137 D Loss: 0.2235, G Loss: 2.7990\n",
      "Epoch [71/100], Batch 0/137 D Loss: 0.1297, G Loss: 3.3675\n",
      "Epoch [71/100], Batch 100/137 D Loss: 0.7141, G Loss: 6.4291\n",
      "Epoch [72/100], Batch 0/137 D Loss: 2.7819, G Loss: 8.1695\n",
      "Epoch [72/100], Batch 100/137 D Loss: 0.2970, G Loss: 4.5635\n",
      "Epoch [73/100], Batch 0/137 D Loss: 0.6119, G Loss: 3.3874\n",
      "Epoch [73/100], Batch 100/137 D Loss: 0.0213, G Loss: 5.4034\n",
      "Epoch [74/100], Batch 0/137 D Loss: 2.9828, G Loss: 9.5462\n",
      "Epoch [74/100], Batch 100/137 D Loss: 0.0334, G Loss: 5.4494\n",
      "Epoch [75/100], Batch 0/137 D Loss: 0.1965, G Loss: 3.3027\n",
      "Epoch [75/100], Batch 100/137 D Loss: 0.2491, G Loss: 3.1277\n",
      "Epoch [76/100], Batch 0/137 D Loss: 3.7933, G Loss: 8.0186\n",
      "Epoch [76/100], Batch 100/137 D Loss: 0.0435, G Loss: 5.2989\n",
      "Epoch [77/100], Batch 0/137 D Loss: 12.8038, G Loss: 2.4792\n",
      "Epoch [77/100], Batch 100/137 D Loss: 0.2087, G Loss: 3.7905\n",
      "Epoch [78/100], Batch 0/137 D Loss: 0.5690, G Loss: 6.5063\n",
      "Epoch [78/100], Batch 100/137 D Loss: 0.1537, G Loss: 3.8868\n",
      "Epoch [79/100], Batch 0/137 D Loss: 5.4213, G Loss: 9.6186\n",
      "Epoch [79/100], Batch 100/137 D Loss: 0.0738, G Loss: 5.2658\n",
      "Epoch [80/100], Batch 0/137 D Loss: 7.1222, G Loss: 5.8509\n",
      "Epoch [80/100], Batch 100/137 D Loss: 0.3138, G Loss: 2.9068\n",
      "Epoch [81/100], Batch 0/137 D Loss: 6.5737, G Loss: 7.6460\n",
      "Epoch [81/100], Batch 100/137 D Loss: 0.4013, G Loss: 2.8147\n",
      "Epoch [82/100], Batch 0/137 D Loss: 1.1526, G Loss: 7.7488\n",
      "Epoch [82/100], Batch 100/137 D Loss: 0.0780, G Loss: 4.9547\n",
      "Epoch [83/100], Batch 0/137 D Loss: 7.0610, G Loss: 6.5468\n",
      "Epoch [83/100], Batch 100/137 D Loss: 0.2432, G Loss: 3.8975\n",
      "Epoch [84/100], Batch 0/137 D Loss: 2.7120, G Loss: 6.8774\n",
      "Epoch [84/100], Batch 100/137 D Loss: 0.0692, G Loss: 5.5430\n",
      "Epoch [85/100], Batch 0/137 D Loss: 0.9075, G Loss: 6.1892\n",
      "Epoch [85/100], Batch 100/137 D Loss: 0.1462, G Loss: 4.9718\n",
      "Epoch [86/100], Batch 0/137 D Loss: 1.1840, G Loss: 8.6649\n",
      "Epoch [86/100], Batch 100/137 D Loss: 0.1319, G Loss: 4.4728\n",
      "Epoch [87/100], Batch 0/137 D Loss: 4.9551, G Loss: 9.9719\n",
      "Epoch [87/100], Batch 100/137 D Loss: 1.3439, G Loss: 2.1234\n",
      "Epoch [88/100], Batch 0/137 D Loss: 3.5430, G Loss: 8.1428\n",
      "Epoch [88/100], Batch 100/137 D Loss: 0.1546, G Loss: 3.3974\n",
      "Epoch [89/100], Batch 0/137 D Loss: 7.9418, G Loss: 3.8843\n",
      "Epoch [89/100], Batch 100/137 D Loss: 0.4215, G Loss: 2.7292\n",
      "Epoch [90/100], Batch 0/137 D Loss: 5.4125, G Loss: 7.6078\n",
      "Epoch [90/100], Batch 100/137 D Loss: 0.1482, G Loss: 3.8136\n",
      "Epoch [91/100], Batch 0/137 D Loss: 0.6344, G Loss: 7.4876\n",
      "Epoch [91/100], Batch 100/137 D Loss: 0.1978, G Loss: 4.3445\n",
      "Epoch [92/100], Batch 0/137 D Loss: 0.4393, G Loss: 6.4196\n",
      "Epoch [92/100], Batch 100/137 D Loss: 0.2312, G Loss: 5.2502\n",
      "Epoch [93/100], Batch 0/137 D Loss: 0.6788, G Loss: 6.6536\n",
      "Epoch [93/100], Batch 100/137 D Loss: 0.0926, G Loss: 3.9740\n",
      "Epoch [94/100], Batch 0/137 D Loss: 7.6710, G Loss: 7.7505\n",
      "Epoch [94/100], Batch 100/137 D Loss: 0.2713, G Loss: 2.8058\n",
      "Epoch [95/100], Batch 0/137 D Loss: 4.9132, G Loss: 8.5942\n",
      "Epoch [95/100], Batch 100/137 D Loss: 0.0791, G Loss: 4.6931\n",
      "Epoch [96/100], Batch 0/137 D Loss: 3.7492, G Loss: 8.2506\n",
      "Epoch [96/100], Batch 100/137 D Loss: 0.1104, G Loss: 5.0117\n",
      "Epoch [97/100], Batch 0/137 D Loss: 3.4141, G Loss: 8.8057\n",
      "Epoch [97/100], Batch 100/137 D Loss: 0.0411, G Loss: 4.8725\n",
      "Epoch [98/100], Batch 0/137 D Loss: 0.1123, G Loss: 3.7004\n",
      "Epoch [98/100], Batch 100/137 D Loss: 0.2905, G Loss: 5.1624\n",
      "Epoch [99/100], Batch 0/137 D Loss: 0.0461, G Loss: 4.1055\n",
      "Epoch [99/100], Batch 100/137 D Loss: 0.0604, G Loss: 5.3488\n",
      "Epoch [100/100], Batch 0/137 D Loss: 0.0457, G Loss: 4.3618\n",
      "Epoch [100/100], Batch 100/137 D Loss: 0.2864, G Loss: 2.4287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /Users/itsme_reddynaveen/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
      "100%|██████████| 91.2M/91.2M [00:01<00:00, 49.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: batch size is bigger than the data size. Setting batch size to data size\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Skip to the next epoch\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m# Calculate FID score\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m fid_value \u001b[39m=\u001b[39m fid_score\u001b[39m.\u001b[39;49mcalculate_fid_given_paths(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     paths\u001b[39m=\u001b[39;49m[REAL_IMAGES_PATH, GENERATED_IMAGES_PATH],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m     dims\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/ImageNET_generating.ipynb#W0sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFID Score: \u001b[39m\u001b[39m{\u001b[39;00mfid_value\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_fid/fid_score.py:259\u001b[0m, in \u001b[0;36mcalculate_fid_given_paths\u001b[0;34m(paths, batch_size, device, dims, num_workers)\u001b[0m\n\u001b[1;32m    255\u001b[0m block_idx \u001b[39m=\u001b[39m InceptionV3\u001b[39m.\u001b[39mBLOCK_INDEX_BY_DIM[dims]\n\u001b[1;32m    257\u001b[0m model \u001b[39m=\u001b[39m InceptionV3([block_idx])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 259\u001b[0m m1, s1 \u001b[39m=\u001b[39m compute_statistics_of_path(paths[\u001b[39m0\u001b[39;49m], model, batch_size,\n\u001b[1;32m    260\u001b[0m                                     dims, device, num_workers)\n\u001b[1;32m    261\u001b[0m m2, s2 \u001b[39m=\u001b[39m compute_statistics_of_path(paths[\u001b[39m1\u001b[39m], model, batch_size,\n\u001b[1;32m    262\u001b[0m                                     dims, device, num_workers)\n\u001b[1;32m    263\u001b[0m fid_value \u001b[39m=\u001b[39m calculate_frechet_distance(m1, s1, m2, s2)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_fid/fid_score.py:243\u001b[0m, in \u001b[0;36mcompute_statistics_of_path\u001b[0;34m(path, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    240\u001b[0m     path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(path)\n\u001b[1;32m    241\u001b[0m     files \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m([file \u001b[39mfor\u001b[39;00m ext \u001b[39min\u001b[39;00m IMAGE_EXTENSIONS\n\u001b[1;32m    242\u001b[0m                    \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m path\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m*.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ext))])\n\u001b[0;32m--> 243\u001b[0m     m, s \u001b[39m=\u001b[39m calculate_activation_statistics(files, model, batch_size,\n\u001b[1;32m    244\u001b[0m                                            dims, device, num_workers)\n\u001b[1;32m    246\u001b[0m \u001b[39mreturn\u001b[39;00m m, s\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_fid/fid_score.py:228\u001b[0m, in \u001b[0;36mcalculate_activation_statistics\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_activation_statistics\u001b[39m(files, model, batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, dims\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m,\n\u001b[1;32m    210\u001b[0m                                     device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    211\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculation of the statistics used by the FID.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m    Params:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m    -- files       : List of image files paths\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m               the inception model.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     act \u001b[39m=\u001b[39m get_activations(files, model, batch_size, dims, device, num_workers)\n\u001b[1;32m    229\u001b[0m     mu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(act, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    230\u001b[0m     sigma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(act, rowvar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_fid/fid_score.py:122\u001b[0m, in \u001b[0;36mget_activations\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    119\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(files)\n\u001b[1;32m    121\u001b[0m dataset \u001b[39m=\u001b[39m ImagePathDataset(files, transforms\u001b[39m=\u001b[39mTF\u001b[39m.\u001b[39mToTensor())\n\u001b[0;32m--> 122\u001b[0m dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataLoader(dataset,\n\u001b[1;32m    123\u001b[0m                                          batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    124\u001b[0m                                          shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    125\u001b[0m                                          drop_last\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    126\u001b[0m                                          num_workers\u001b[39m=\u001b[39;49mnum_workers)\n\u001b[1;32m    128\u001b[0m pred_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((\u001b[39mlen\u001b[39m(files), dims))\n\u001b[1;32m    130\u001b[0m start_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:389\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    385\u001b[0m             sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m batch_sampler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     batch_sampler \u001b[39m=\u001b[39m BatchSampler(sampler, batch_size, drop_last)\n\u001b[1;32m    391\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_last \u001b[39m=\u001b[39m drop_last\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/sampler.py:324\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    311\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    312\u001b[0m     sampler: Union[Sampler[\u001b[39mint\u001b[39m], Iterable[\u001b[39mint\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39m# check here.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    320\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mint\u001b[39m)\n\u001b[1;32m    321\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mbool\u001b[39m)\n\u001b[1;32m    322\u001b[0m         \u001b[39mor\u001b[39;00m batch_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    323\u001b[0m     ):\n\u001b[0;32m--> 324\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch_size should be a positive integer value, but got batch_size=\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         )\n\u001b[1;32m    327\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(drop_last, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    328\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdrop_last should be a boolean value, but got drop_last=\u001b[39m\u001b[39m{\u001b[39;00mdrop_last\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "# Set your dataset path\n",
    "DATASET_DIR = r\"/Users/itsme_reddynaveen/Desktop/Shylendra_imagenet/sorted/\"\n",
    "REAL_IMAGES_PATH = \"./real_images2/\"\n",
    "GENERATED_IMAGES_PATH = \"./generated_images2/\"\n",
    "os.makedirs(REAL_IMAGES_PATH, exist_ok=True)\n",
    "os.makedirs(GENERATED_IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "num_epochs = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(DATASET_DIR, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# DCGAN Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# DCGAN Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Training loop with try-except block\n",
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "            # Prepare real and fake labels\n",
    "            real_imgs = imgs.to(device)\n",
    "            real_labels = torch.ones(imgs.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(imgs.size(0), 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            output_real = discriminator(real_imgs)\n",
    "            real_loss = criterion(output_real, real_labels)\n",
    "            z = torch.randn(imgs.size(0), latent_dim, 1, 1, device=device)\n",
    "            fake_imgs = generator(z)\n",
    "            output_fake = discriminator(fake_imgs.detach())\n",
    "            fake_loss = criterion(output_fake, fake_labels)\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            output_fake = discriminator(fake_imgs)\n",
    "            gen_loss = criterion(output_fake, real_labels)\n",
    "            gen_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Logging\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch {i}/{len(dataloader)} \"\n",
    "                      f\"D Loss: {d_loss.item():.4f}, G Loss: {gen_loss.item():.4f}\")\n",
    "\n",
    "        # Save generated samples\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(64, latent_dim, 1, 1, device=device)\n",
    "            samples = generator(z)\n",
    "            samples = (samples + 1) / 2  # Rescale to [0, 1]\n",
    "            save_image(samples, f\"{GENERATED_IMAGES_PATH}/epoch_{epoch+1}.png\", nrow=8)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found during training: {e}\")\n",
    "        continue  # Skip to the next epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate FID score\n",
    "fid_value = fid_score.calculate_fid_given_paths(\n",
    "    paths=[REAL_IMAGES_PATH, GENERATED_IMAGES_PATH],\n",
    "    batch_size=50,\n",
    "    device=device,\n",
    "    dims=2048,\n",
    ")\n",
    "print(f\"FID Score: {fid_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
